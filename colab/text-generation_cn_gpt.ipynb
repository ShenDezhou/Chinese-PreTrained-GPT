{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19175,"status":"ok","timestamp":1686882580148,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"},"user_tz":-480},"id":"2npF9Mw3NtAs","outputId":"642e860e-c8cb-499f-eb1c-7e24b49a0683"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"]}],"source":["!pip install -U pip\n","!pip install -q tensorflow\n","!pip install -q transformers\n","!pip install -q torch"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code\n","#!wget https://transformers-models.obs.cn-north-4.myhuaweicloud.com/gpt/cn/pretrain/gpt2_L-12_H-768_A-12_CN.zip\n","!unzip gpt2_L-12_H-768_A-12_CN.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0i0or6XAM1V","executionInfo":{"status":"ok","timestamp":1686882082904,"user_tz":-480,"elapsed":7027,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"1e321863-0917-4be0-d64d-e3ed1b7e6f33"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code\n","Archive:  gpt2_L-12_H-768_A-12_CN.zip\n","   creating: gpt2_L-12_H-768_A-12_CN/\n","  inflating: gpt2_L-12_H-768_A-12_CN/config.json  \n","  inflating: gpt2_L-12_H-768_A-12_CN/vocab.txt  \n","  inflating: gpt2_L-12_H-768_A-12_CN/pytorch_model.bin  \n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BSDVm1SN0nn","executionInfo":{"status":"ok","timestamp":1686882168957,"user_tz":-480,"elapsed":2701,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"00d396a3-f30f-43d1-ee86-abcde4db4c44"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]}],"source":["import tensorflow as tf\n","import torch\n","from transformers import GPT2LMHeadModel, BertTokenizer\n","\n","\n","tokenizer = BertTokenizer.from_pretrained(\"gpt2_L-12_H-768_A-12_CN\")\n","\n","# add the EOS token as PAD token to avoid warnings\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2_L-12_H-768_A-12_CN\", pad_token_id=0)"]},{"cell_type":"markdown","metadata":{"id":"KrYirCIEN8x7"},"source":["#Greedy Search\n","Starting from the word\n","\"The\", the algorithm greedily chooses the next word of highest probability\n","\"nice\" and so on, so that the final generated word sequence is\n","\n","(\"The\",\"nice\",\"woman\") having an overall probability of\n","\n","0.5Ã—0.4=0.2 ."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"V7jGarSpN75l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686882360569,"user_tz":-480,"elapsed":3054,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"c2fa3949-368d-4cb6-d765-db1edb4d6c35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ äºº å’« å°º èµ· å¤© æ¶¯ ã€‚ äº‘ é—´ è‹¥ æœ‰ æ‹› å·¢ å£« ï¼Œ å¥½ çœ‹ åµ‡ ç”Ÿ å‰Š é¹„ å ã€‚ è¯´ å¤© è¾¹ äº§ å® å³° ï¼Œ äº‘ éœ å½· ä½› æ´ å¤© ç©º ã€‚ è«\n"]}],"source":["# encode context the generation is conditioned on\n","input_ids = tokenizer.encode('è¿œä¸Šå¯’å±±çŸ³å¾„æ–œï¼Œ', return_tensors='pt')\n","\n","# generate text until the output length (which includes the context length) reaches 50\n","greedy_output = model.generate(input_ids, max_length=50)\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"]},{"cell_type":"markdown","source":["The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search - check out Vijayakumar et al., 2016 and Shao et al., 2017.\n","\n","The word\n","\"has\" with its high conditional probability of\n","0.9 is hidden behind the word\n","\"dog\", which has only the second-highest conditional probability, so that greedy search misses the word sequence\n","\"The\",\"dog\",\"has\" ."],"metadata":{"id":"SCLZIT1c84ZZ"}},{"cell_type":"markdown","source":["#Beam Search"],"metadata":{"id":"0cnnF136RVnB"}},{"cell_type":"code","source":["# activate beam search and early_stopping\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mePnmWDkRXYP","executionInfo":{"status":"ok","timestamp":1686882378454,"user_tz":-480,"elapsed":3374,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"d15ecbd7-4af1-45cc-ce44-2bbc544cfc3a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ é’\n"]}]},{"cell_type":"markdown","source":["Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.\n","\n","Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n","\n","Let's see how beam search can be used in transformers. We set num_beams > 1 and early_stopping=True so that generation is finished when all beam hypotheses reached the EOS token."],"metadata":{"id":"Lnr782009SY7"}},{"cell_type":"markdown","source":["#Fluency"],"metadata":{"id":"-PREtRBoRbJw"}},{"cell_type":"markdown","source":["While the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n","A simple remedy is to introduce n-grams (a.k.a word sequences of n words) penalties as introduced by Paulus et al. (2017) and Klein et al. (2017). The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0."],"metadata":{"id":"U0hvhTOz9wdN"}},{"cell_type":"code","source":["# set no_repeat_ngram_size to 2\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaseclqyRc7f","executionInfo":{"status":"ok","timestamp":1686882411844,"user_tz":-480,"elapsed":4397,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"bbf354c4-873b-4747-b6f1-69a9c4b52ef1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ é’\n"]}]},{"cell_type":"markdown","source":["Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.\n","\n","In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences <= num_beams!"],"metadata":{"id":"LVl6gGLl-MlC"}},{"cell_type":"code","source":["# set return_num_sequences > 1\n","beam_outputs = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    num_return_sequences=5,\n","    early_stopping=True\n",")\n","\n","# now we have 3 output sequences\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HFwXdWY-Ir4","executionInfo":{"status":"ok","timestamp":1686882423309,"user_tz":-480,"elapsed":3429,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"42fd7145-cbff-4dc5-b13a-64a419320fff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ é’\n","1: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ çœŸ\n","2: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ ç›¸\n","3: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ ç§‹\n","4: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å·² æ—  å®¶ ã€‚ å å¹´ ä¸€ è§‰ æ²³ å±± æ¢¦ ï¼Œ ç‹¬ ç¾¡ éš å· å‡  æ ‘ èŠ± ã€‚ æƒ³ å¤© å° ä¸ å¹¿ å¯’ ï¼Œ ç¼ æ¥¼ ç‰ å®‡ è·¯ æ¼« æ¼« ã€‚ å¤š\n"]}]},{"cell_type":"markdown","source":["In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n","\n","Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization - see Murray et al. (2018) and Yang et al. (2018). But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n","\n","We have seen that beam search heavily suffers from repetitive generation. This is especially hard to control with n-gram- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical n-grams requires a lot of finetuning.\n","\n","As argued in Ari Holtzman et al. (2019), high quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. The authors show this nicely by plotting the probability, a model would give to human text vs. what beam search does."],"metadata":{"id":"5LzbFltj-7aC"}},{"cell_type":"markdown","source":["#Sampling\n","\n","In its most basic form, sampling means randomly picking the next word\n","  according to its conditional probability distribution:\n","\n","In transformers, we set do_sample=True and deactivate Top-K sampling (more on this later) via top_k=0. In the following, we will fix random_seed=0 for illustration purposes. Feel free to change the random_seed to play around with the model.\n","\n","Interesting! The text seems alright - but when taking a closer look, it is not very coherent. the 3-grams new hand sense and local batte harness are very weird and don't sound like they were written by a human. That is the big problem when sampling word sequences: The models often generate incoherent gibberish, cf. Ari Holtzman et al. (2019)."],"metadata":{"id":"Z_b6xJjQ_Aut"}},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","torch.random.manual_seed(0)\n","\n","# activate sampling and deactivate top_k by setting top_k sampling to 0\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 50 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9EIw4Y--8ZR","executionInfo":{"status":"ok","timestamp":1686882656586,"user_tz":-480,"elapsed":2834,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"7d9842db-35db-4607-af21-fedfff775403"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","--------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ è¾¹ åˆš ä½ é‡ äºº å®¶ ã€‚ ç«¹ ç¬¼ å®¿ éœ² ç”Ÿ ç§‹ å ï¼Œ æœ¨ ç¬” æ˜¥ é£ èµ· é˜µ æ–œ ã€‚ å¦‡ å¥³ æ®‹ å¦† æºª æ¶§ ä¾§ ï¼Œ æ¨µ è‹ å½’ æ‹… é‡ ç”° æ–œ ã€‚ é“\n"]}]},{"cell_type":"markdown","source":["A trick is to make the distribution\n","sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called temperature of the softmax.\n","\n","An illustration of applying temperature to our example from above could look as follows."],"metadata":{"id":"ihcYJlYfAVo3"}},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","torch.random.manual_seed(0)\n","\n","# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0,\n","    temperature=0.7\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZo9xwywAcw_","executionInfo":{"status":"ok","timestamp":1686882672954,"user_tz":-480,"elapsed":2799,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"eb399b89-d5b4-4b1f-e26c-64afe5f654cb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ è¾¹ åˆš ä½ é‡ äºº å®¶ ã€‚ ç«¹ ç¬¼ å®¿ é›¾ æ‚¬ ç§‹ æœˆ ï¼Œ æ¾ é˜ é’Ÿ å£° ä¸‹ æ™“ ã€‚ è¡Œ å°½ æ°´ ä¹¡ äºº ä¸ è§ ï¼Œ å¯» æ¥ åƒ§ èˆ å®¢ çŠ¹ èµŠ ã€‚ è¯·\n"]}]},{"cell_type":"markdown","source":["#Top-K Sampling\n","Fan et. al (2018) introduced a simple, but very powerful sampling scheme, called Top-K sampling. In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation.\n","\n","We extend the range of words used for both sampling steps in the example above from 3 words to 10 words to better illustrate Top-K sampling."],"metadata":{"id":"yJ-LflHZAp9-"}},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","torch.random.manual_seed(0)\n","\n","# set top_k to 50\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbExeqdPA3gN","executionInfo":{"status":"ok","timestamp":1686882692461,"user_tz":-480,"elapsed":2046,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"e7f057b0-e25f-4c49-edd3-25cbbf870b9c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ è¾¹ åˆš ä½ é‡ äºº å®¶ ã€‚ ç«¹ ç¬¼ å®¿ éœ² ç”Ÿ ç§‹ å ï¼Œ è‰ æµ è¤ æ‹¾ æ¶§ èŠ± ã€‚ éš” å æ³‰ å£° å¦‚ éš éš ï¼Œ éš” æ— é£ é€ ä¼¼ äº‘ éœ ã€‚ ç™½\n"]}]},{"cell_type":"markdown","source":["In step\n","\n","t=1, Top-K eliminates the possibility to sample\n","\n","(\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. On the other hand, in step\n","\n","t=2 the method includes the arguably ill-fitted words\n","\n","(\"down\",\"a\") in the sample pool of words. Thus, limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. This intuition led Ari Holtzman et al. (2019) to create Top-p- or nucleus-sampling."],"metadata":{"id":"iW49vgDkBOs1"}},{"cell_type":"markdown","source":["#Top-p (nucleus) sampling\n","Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize.\n"],"metadata":{"id":"yjnf09CtBV5U"}},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","torch.random.manual_seed(0)\n","\n","# deactivate top_k sampling and sample only from 92% most likely words\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_p=0.92,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsL-DBZxBXL1","executionInfo":{"status":"ok","timestamp":1686882702802,"user_tz":-480,"elapsed":2841,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"b25ab132-87b4-4fdb-ab6d-48d0d9247c09"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ è¾¹ åˆš ä½ é‡ äºº å®¶ ã€‚ ç«¹ ç¬¼ å®¿ éœ² ç”Ÿ ç§‹ è‰² ï¼Œ æ¾ é—ª è½» é£ ä¼´ æ—¥ å ã€‚ æµ· æ›² äº‰ å…ˆ äºº å» å°‘ ï¼Œ ä¼Š å· ä¸ å¾… å®¢ æ€ é ã€‚ å¥½\n"]}]},{"cell_type":"markdown","source":["Great, that sounds like it could have been written by a human. Well, maybe not quite yet.\n","\n","While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection.\n","\n","Finally, to get multiple independently sampled outputs, we can again set the parameter num_return_sequences > 1:"],"metadata":{"id":"yf9nRCmcBy19"}},{"cell_type":"code","source":["# set seed to reproduce results. Feel free to change the seed though to get different results\n","torch.random.manual_seed(0)\n","\n","# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=50,\n","    top_p=0.95,\n","    num_return_sequences=3\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYASjtLfBzvd","executionInfo":{"status":"ok","timestamp":1686882715998,"user_tz":-480,"elapsed":3520,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"6b2f1319-a49f-43bf-f5a2-f7f7fb039ce7"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ è¾¹ åˆš ä½ é‡ äºº å®¶ ã€‚ ç«¹ ç¬¼ å®¿ éœ² ç”Ÿ ç§‹ è‰² ï¼Œ æ¾ é—ª è½» é£ ä¼´ æ—¥ å ã€‚ æµ· æ›² äº‰ å…ˆ äºº å» å°‘ ï¼Œ èŒ… æª ç›¸ è¿‘ é¸Ÿ å•¼ å“— ã€‚ å¥½\n","1: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ äºº æƒ† æ€… æœª è¿˜ å®¶ ã€‚ ä¸ è¾ æ»¡ é…Œ é‡ é˜³ é…’ ï¼Œ å¤š å°‘ æ¸… æ˜ æœª è½ èŠ± ã€‚ ç¬‘ å£° å åŠ¨ äº¬ æ´› ï¼Œ æ¬² å°† èº« ä¸– é—® çƒŸ éœ ã€‚ æ•…\n","2: è¿œ ä¸Š å¯’ å±± çŸ³ å¾„ æ–œ ï¼Œ å¹³ ä»Š æ—¥ å¢“ ç”° å®¶ ã€‚ é£ äº‘ æƒ¨ æ¾¹ æ„ çœ‰ é›¨ ï¼Œ è‰ æœ¨ è’ å‡‰ å®¿ å¤œ ã€‚ ä½• æ—¥ é“­ åŠŸ å½’ å¤œ å¸ ï¼Œ è° äºº æ‰¶ ç«‹ çœ‹ æ§ èŠ± ã€‚ ä¸€\n"]}]},{"cell_type":"markdown","source":["#Conclusion\n","As ad-hoc decoding methods, top-p and top-K sampling seem to produce more fluent text than traditional greedy - and beam search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of greedy and beam search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, cf. Welleck et al. (2019). Also, as demonstrated in Welleck et al. (2020), it looks as top-K and top-p sampling also suffer from generating repetitive word sequences.\n","\n","In Welleck et al. (2019), the authors show that according to human evaluations, beam search can generate more fluent text than Top-p sampling, when adapting the model's training objective.\n","\n","Open-ended language generation is a rapidly evolving field of research and as it is often the case there is no one-size-fits-all method here, so one has to see what works best in one's specific use case.\n","\n","Good thing, that you can try out all the different decoding methods in transfomers ğŸ¤—.\n","\n","That was a short introduction on how to use different decoding methods in transformers and recent trends in open-ended language generation.\n","\n","Feedback and questions are very welcome on the Github repository.\n","\n","For more fun generating stories, please take a look at Writing with Transformers\n","\n","Thanks to everybody, who has contributed to the blog post: Alexander Rush, Julien Chaumand, Thomas Wolf, Victor Sanh, Sam Shleifer, ClÃ©ment Delangue, Yacine Jernite, Oliver Ã…strand and John de Wasseige.\n","\n","Appendix\n","There are a couple of additional parameters for the generate method that were not mentioned above. We will explain them here briefly!\n","\n","min_length can be used to force the model to not produce an EOS token (= not finish the sentence) before min_length is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n","\n","repetition_penalty can be used to penalize words that were already generated or belong to the context. It was first introduced by Keskar et al. (2019) and is also used in the training objective in Welleck et al. (2019). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, e.g. see this discussion on Github.\n","\n","attention_mask can be used to mask padded tokens\n","\n","pad_token_id, bos_token_id, eos_token_id: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n","\n","For more information please also look into the generate function docstring."],"metadata":{"id":"hbdqZNB5Cd4K"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1rb6Jpsnomz2Zy__RVZ6N4qdRRV54-UjT","authorship_tag":"ABX9TyMTSsWuKNTC5CYW6FxH80Xr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}